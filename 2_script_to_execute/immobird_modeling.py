# =======================================================================================================================================
# ImmoBird - Pipeline Complet de Mod√©lisation
# Pr√©traitement ‚Üí Entra√Ænement XGBoost ‚Üí Rapport
# ---------------------------------------------------------------------------------------------------------------------------------------
# Pipeline p√©dagogique pour CDP Data & IA : d√©monstration bout-en-bout
# Sans red flags, propre, comment√© pour apprentissage
# =======================================================================================================================================

import os                          # Gestion des chemins et fichiers syst√®me
import hashlib                     # Calcul de checksums pour d√©tection de drift
import json                        # S√©rialisation des donn√©es pour rapports
from datetime import datetime      # Horodatage des ex√©cutions

import pandas as pd                                                 # Manipulation des donn√©es tabulaires
import numpy as np                                                  # Calculs num√©riques et m√©triques
from sklearn.preprocessing import StandardScaler, OneHotEncoder     # Normalisation et encodage
from sklearn.model_selection import train_test_split                # Division train/test
from sklearn.metrics import mean_squared_error, r2_score            # M√©triques d'√©valuation
import xgboost as xgb                                               # Algorithme de machine learning (gradient boosting)
import joblib                                                       # S√©rialisation des mod√®les ML


# =======================================================================================================================================
# CONFIGURATION GLOBALE
# =======================================================================================================================================

DATA_DIR = "/opt/airflow/data"      # R√©pertoire des donn√©es (brutes et transform√©es)
REPORT_DIR = "/opt/airflow/reports" # R√©pertoire des rapports g√©n√©r√©s


# =======================================================================================================================================
# FONCTIONS UTILITAIRES
# =======================================================================================================================================

def sha256_file(path: str) -> str:
    """Calcule le hash SHA256 d'un fichier pour d√©tecter les changements de donn√©es."""
    h = hashlib.sha256()                               # Initialisation du hash SHA256
    with open(path, "rb") as f:                    # Ouverture en mode binaire
        for chunk in iter(lambda: f.read(8192), b""):  # Lecture par chunks de 8KB
            h.update(chunk)                            # Mise √† jour du hash
    return h.hexdigest()                               # Retour du hash en hexad√©cimal


# =======================================================================================================================================
# √âTAPE 1 : PR√âTRAITEMENT DES DONN√âES
# =======================================================================================================================================

def preprocess_data():
    """√âtape 1: Nettoyage, transformation et pr√©paration des donn√©es brutes pour le ML."""
    print("=== √âTAPE 1: PR√âTRAITEMENT ===")
    
    # Chargement des donn√©es brutes depuis le fichier CSV
    df = pd.read_csv(os.path.join(DATA_DIR, 'house_pred.csv'))
    df_ml = df.copy()                              # Copie de travail pour √©viter de modifier l'original
    
    # Diagnostic : v√©rification des valeurs manquantes
    print("Valeurs manquantes par colonne :")
    print(df_ml.isnull().sum())
    
    # Strat√©gie de traitement des valeurs manquantes
    # Pour les colonnes num√©riques : remplacement par la m√©diane (robuste aux outliers)
    numeric_columns = ['Area', 'YearBuilt']
    for col in numeric_columns:
        median_value = df_ml[col].median()         # Calcul de la m√©diane
        df_ml[col].fillna(median_value, inplace=True)  # Remplacement des NaN
    
    # Pour les colonnes cat√©gorielles : remplacement par le mode (valeur la plus fr√©quente)
    categorical_columns = ['Location', 'Garage', 'Condition']
    for col in categorical_columns:
        mode_value = df_ml[col].mode()[0]          # R√©cup√©ration de la valeur la plus fr√©quente
        df_ml[col].fillna(mode_value, inplace=True)    # Remplacement des NaN
    
    # Normalisation des variables num√©riques (centrer-r√©duire)
    scaler = StandardScaler()                      # Initialisation du normalisateur
    df_ml[['Area', 'YearBuilt']] = scaler.fit_transform(df_ml[['Area', 'YearBuilt']])
    
    # Encodage des variables cat√©gorielles en variables binaires (One-Hot Encoding)
    ohe = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' √©vite la multicolin√©arit√©
    all_cat_encoded = ohe.fit_transform(df_ml[['Location', 'Garage', 'Condition']])
    
    # Construction des noms de colonnes pour les variables encod√©es
    location_categories = ohe.categories_[0][1:]   # R√©cup√©ration des cat√©gories (sans la premi√®re)
    garage_categories = ohe.categories_[1][1:]
    condition_categories = ohe.categories_[2][1:]
    new_columns = [f'Location_{cat}' for cat in location_categories] + \
                  [f'Garage_{cat}' for cat in garage_categories] + \
                  [f'Condition_{cat}' for cat in condition_categories]
    
    # Cr√©ation d'un DataFrame avec les variables encod√©es
    encoded_df = pd.DataFrame(all_cat_encoded, columns=new_columns)
    
    # Fusion des donn√©es : suppression des colonnes cat√©gorielles originales + ajout des encod√©es
    df_ml = pd.concat([
        df_ml.drop(['Location', 'Garage', 'Condition'], axis=1),  # Suppression des originales
        encoded_df                                 # Ajout des variables binaires
    ], axis=1)
    
    # Sauvegarde du jeu de donn√©es pr√©trait√© pour l'√©tape suivante
    output_path = os.path.join(DATA_DIR, 'house_pred_for_ml.csv')
    df_ml.to_csv(output_path, index=False)         # Sauvegarde sans l'index
    
    # Affichage des r√©sultats pour validation
    print("\nPr√©traitement termin√© !")
    print("\nAper√ßu des donn√©es pr√©trait√©es :")
    print(df_ml.head())                            # Affichage des 5 premi√®res lignes
    print("\nInformations sur les colonnes :")
    print(df_ml.info())                            # Structure du DataFrame
    
    return df_ml


# =======================================================================================================================================
# √âTAPE 2 : ENTRA√éNEMENT DU MOD√àLE
# =======================================================================================================================================

def train_model():
    """√âtape 2: Entra√Ænement d'un mod√®le XGBoost et √©valuation des performances."""
    print("\n=== √âTAPE 2: ENTRA√éNEMENT ===")
    
    # Chargement des donn√©es pr√©trait√©es depuis l'√©tape pr√©c√©dente
    df = pd.read_csv(os.path.join(DATA_DIR, 'house_pred_for_ml.csv'))
    
    # S√©paration des variables explicatives (X) et de la variable cible (y)
    X = df.drop('Price', axis=1)                   # Features : toutes les colonnes sauf Price
    y = df['Price']                                # Target : la colonne Price √† pr√©dire
    
    # Division stratifi√©e du dataset (80% entra√Ænement, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=0.2,                             # 20% pour le test
        random_state=42                            # Reproductibilit√© des r√©sultats
    )
    
    # Configuration et initialisation du mod√®le XGBoost
    model = xgb.XGBRegressor(
        n_estimators=100,                          # Nombre d'arbres (plus = mieux mais plus lent)
        learning_rate=0.1,                         # Taux d'apprentissage (compromis vitesse/pr√©cision)
        max_depth=5,                               # Profondeur max des arbres (√©vite le surapprentissage)
        min_child_weight=1,                        # Poids minimum par feuille (r√©gularisation)
        subsample=0.8,                             # Fraction d'√©chantillons par arbre (√©vite le surapprentissage)
        colsample_bytree=0.8,                      # Fraction de features par arbre (diversification)
        random_state=42                            # Reproductibilit√©
    )
    
    # Entra√Ænement du mod√®le sur les donn√©es d'entra√Ænement
    model.fit(X_train, y_train)                   # Apprentissage des patterns prix/features
    
    # Pr√©diction sur l'ensemble de test (donn√©es non vues pendant l'entra√Ænement)
    y_pred = model.predict(X_test)                # Pr√©dictions du mod√®le
    
    # Calcul des m√©triques de performance
    mse = mean_squared_error(y_test, y_pred)      # Erreur quadratique moyenne
    rmse = np.sqrt(mse)                           # Racine de l'erreur quadratique (m√™me unit√© que Price)
    r2 = r2_score(y_test, y_pred)                 # Coefficient de d√©termination (% variance expliqu√©e)
    
    # Affichage des r√©sultats d'√©valuation
    print("\nR√©sultats de l'√©valuation du mod√®le :")
    print(f"RMSE : {rmse:.2f}")                   # Plus c'est bas, mieux c'est
    print(f"R¬≤ : {r2:.4f}")                       # Plus c'est proche de 1, mieux c'est
    
    # Sauvegarde des m√©triques dans un fichier pour tra√ßabilit√©
    metrics_path = os.path.join(DATA_DIR, "metrics.txt")
    with open(metrics_path, "w", encoding="utf-8") as f:
        f.write(f"RMSE: {rmse:.2f}\nR2: {r2:.4f}\n")  # Format structur√© pour parsing
    
    # Analyse de l'importance des variables (quelles features influencent le plus le prix)
    feature_importance = pd.DataFrame({
        'Feature': X.columns,                      # Noms des features
        'Importance': model.feature_importances_   # Scores d'importance XGBoost
    })
    feature_importance = feature_importance.sort_values('Importance', ascending=False)  # Tri d√©croissant
    print("\nImportance des features :")
    print(feature_importance)                     # Aide √† comprendre les drivers de prix
    
    # Persistance du mod√®le entra√Æn√© pour r√©utilisation future
    model_path = os.path.join(DATA_DIR, 'immo_model.joblib')
    joblib.dump(model, model_path)                # S√©rialisation du mod√®le
    
    # D√©monstration : exemple de pr√©diction sur un cas r√©el
    test_features = X_test.iloc[0].to_dict()                           # Premier exemple du jeu de test
    predicted_price = model.predict(pd.DataFrame([test_features]))[0]  # Pr√©diction
    actual_price = y_test.iloc[0]                                      # Prix r√©el correspondant 
    
    print("\nExemple de pr√©diction :")
    print(f"Prix pr√©dit : {predicted_price:.2f}")
    print(f"Prix r√©el : {actual_price:.2f}")
    
    return rmse, r2, feature_importance


# =======================================================================================================================================
# √âTAPE 3 : G√âN√âRATION DE RAPPORT
# =======================================================================================================================================

def generate_report():
    """√âtape 3: G√©n√©ration d'un rapport Markdown avec m√©triques et recommandations CDP."""
    print("\n=== √âTAPE 3: RAPPORT ===")
    
    # Cr√©ation du r√©pertoire de rapports s'il n'existe pas
    os.makedirs(REPORT_DIR, exist_ok=True)
    
    # R√©cup√©ration des m√©tadonn√©es d'ex√©cution depuis les variables d'environnement Airflow
    dag_run_id = os.environ.get("AIRFLOW_CTX_DAG_RUN_ID", "manual")                               # ID de l'ex√©cution DAG
    execution_date = os.environ.get("AIRFLOW_CTX_EXECUTION_DATE", datetime.utcnow().isoformat())  # Date d'ex√©cution
    image = os.environ.get("IMAGE_NAME", "immo-demo:latest")                                      # Image Docker utilis√©e
    git_sha = os.environ.get("GIT_SHA", "unknown")                                                # Hash du code source
    
    # D√©finition des chemins vers les fichiers de donn√©es
    raw_csv = os.path.join(DATA_DIR, "house_pred.csv")                      # Donn√©es brutes
    ml_csv = os.path.join(DATA_DIR, "house_pred_for_ml.csv")                # Donn√©es pr√©trait√©es
    metrics_path = os.path.join(DATA_DIR, "metrics.txt")                    # M√©triques du mod√®le
    
    # Chargement des datasets pour analyse
    df_raw = pd.read_csv(raw_csv)                                           # Dataset original
    df_ml = pd.read_csv(ml_csv)                                             # Dataset transform√©
    
    # Calcul des m√©tadonn√©es du dataset
    dataset_size_bytes = os.path.getsize(raw_csv)                                      # Taille du fichier en bytes
    num_features = df_ml.shape[1] - 1 if "Price" in df_ml.columns else df_ml.shape[1]  # Nombre de features
    
    # Analyse de qualit√© des donn√©es
    missing_counts = df_raw.isnull().sum().to_dict()                        # Comptage des valeurs manquantes
    target = df_raw.get("Price")                                            # Variable cible
    target_mean = float(target.mean()) if target is not None else None      # Moyenne des prix
    target_std = float(target.std()) if target is not None else None        # √âcart-type des prix
    
    # Calcul du checksum pour d√©tecter les changements de donn√©es (data drift)
    checksum = sha256_file(raw_csv)                                         # Hash du fichier de donn√©es
    drift_score = float(abs((target_mean or 0) / (target_std or 1)))        # Score de drift simplifi√©
    
    # Lecture des m√©triques de performance du mod√®le
    rmse = None                                                             # Root Mean Square Error
    r2 = None                                                               # Coefficient de d√©termination
    if os.path.exists(metrics_path):                                        # Si le fichier de m√©triques existe
        with open(metrics_path, "r", encoding="utf-8") as f:
            for line in f:                                                  # Parsing ligne par ligne
                if line.startswith("RMSE:"):                                # Extraction du RMSE
                    rmse = float(line.split(":")[1].strip())
                if line.startswith("R2:") or line.startswith("R¬≤:"):        # Extraction du R¬≤
                    try:
                        r2 = float(line.split(":")[1].strip())
                    except Exception:
                        pass                                                # Gestion des erreurs de parsing
    
    # Identification des top-5 features (simplifi√©e pour la d√©mo)
    top5_features = [c for c in df_ml.columns if c != "Price"][:5]          # Premi√®res features (hors Price)
    
    # G√©n√©ration du nom de fichier rapport avec timestamp
    now_str = datetime.utcnow().strftime("%Y%m%d_%H%M%S")                   # Format: YYYYMMDD_HHMMSS
    report_name = f"report_{now_str}_{dag_run_id}.md"                       # Nom unique du rapport
    report_path = os.path.join(REPORT_DIR, report_name)                     # Chemin complet
    
    # Recommandations pro-forma pour les CDP (exemples p√©dagogiques)
    decisions = [
        "√† surveiller: stabilit√© de la variable cible",                     # Monitoring continu
        "action recommand√©e: planifier un rafra√Æchissement des donn√©es",    # Maintenance donn√©es
        "prochain incr√©ment: ajouter des features temporelles",             # √âvolution mod√®le
    ]
    
    # Construction du contenu Markdown du rapport
    content = []
    content.append(f"# Rapport d'ex√©cution ‚Äî {dag_run_id}")                # Titre avec ID d'ex√©cution
    content.append("")
    content.append("## M√©tadonn√©es")                                       # Section m√©tadonn√©es
    content.append(f"- dag_run_id: `{dag_run_id}`")                        # Identifiant de l'ex√©cution
    content.append(f"- date/heure: `{execution_date}`")                    # Horodatage
    content.append(f"- image Docker: `{image}`")                           # Version de l'environnement
    content.append(f"- hash du code (GIT_SHA): `{git_sha}`")               # Version du code
    content.append(f"- taille dataset (bytes): `{dataset_size_bytes}`")    # Taille des donn√©es
    content.append(f"- nb de features: `{num_features}`")                  # Dimensionnalit√©
    content.append("")
    content.append("## Qualit√© & d√©rive")                                                      # Section qualit√© donn√©es
    content.append(f"- nb de valeurs manquantes (raw): `{json.dumps(missing_counts)}`")        # Donn√©es manquantes
    content.append(f"- cible (Price) ‚Äî moyenne: `{target_mean}`, √©cart-type: `{target_std}`")  # Stats cible
    content.append(f"- checksum CSV: `{checksum}`")                                            # Empreinte des donn√©es
    content.append(f"- drift score (p√©dagogique): `{drift_score:.4f}`")                        # Indicateur de d√©rive
    content.append("")
    content.append("## R√©sultats mod√®le")                                  # Section performance
    content.append(f"- RMSE: `{rmse}`")                                    # Erreur de pr√©diction
    content.append(f"- R¬≤: `{r2}`")                                        # Qualit√© d'ajustement
    content.append(f"- Top-5 features: `{top5_features}`")                 # Variables importantes
    content.append("")
    content.append("## D√©cisions CDP (pro-forma)")                          # Section recommandations
    for d in decisions:                                                     # Ajout de chaque recommandation
        content.append(f"- {d}")
    
    # √âcriture du rapport dans le fichier Markdown
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("\n".join(content) + "\n")                                 # Assemblage et sauvegarde
    
    print(f"Report generated at: {report_path}")                           # Confirmation de g√©n√©ration
    
    # Fonctionnalit√© optionnelle : push vers un repository GitHub
    repo_url = os.environ.get("REPORTS_REPO_URL")                              # URL du repo (si d√©finie)
    repo_branch = os.environ.get("REPORTS_REPO_BRANCH", "main")                # Branche cible
    if repo_url:                                                               # Si un repo est configur√©
        os.system("git config --global user.email 'airflow-bot@example.com'")  # Config Git
        os.system("git config --global user.name 'airflow-bot'")
        os.chdir("/opt/airflow")                                                # Changement de r√©pertoire
        os.system("git init")                                                   # Initialisation Git
        os.system("git checkout -B " + repo_branch)                             # Cr√©ation/switch branche
        os.system("git add reports/")                                           # Ajout des rapports
        os.system("git commit -m 'Add report '" + report_name + " || true")     # Commit (ignore √©checs)
        os.system("git remote remove origin 2>/dev/null || true")               # Suppression remote existant
        os.system("git remote add origin '" + repo_url + "'")                   # Ajout nouveau remote
        os.system("git push -u origin " + repo_branch + " --force")             # Push forc√©


# =======================================================================================================================================
# ORCHESTRATION PRINCIPALE
# =======================================================================================================================================

def main():
    """Orchestration compl√®te du pipeline : pr√©traitement ‚Üí entra√Ænement ‚Üí rapport."""
    print("D√âMARRAGE DU PIPELINE IMMOBIRD MODELING")
    print("="*80)
    
    try:
        # √âtape 1: Nettoyage et pr√©paration des donn√©es brutes
        print("üîÑ Lancement de l'√©tape 1...")
        df_processed = preprocess_data()                    # Transformation des donn√©es brutes
        
        # √âtape 2: Entra√Ænement du mod√®le de machine learning
        print("üîÑ Lancement de l'√©tape 2...")
        rmse, r2, feature_importance = train_model()        # Entra√Ænement et √©valuation XGBoost
        
        # √âtape 3: G√©n√©ration du rapport de pilotage pour les CDP
        print("üîÑ Lancement de l'√©tape 3...")
        generate_report()                                   # Cr√©ation du rapport Markdown
        
        # R√©sum√© final pour validation
        print("\n" + "="*80)
        print("‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS")
        print(f"üìä M√©triques finales: RMSE={rmse:.2f}, R¬≤={r2:.4f}")  # Affichage des performances
        print("üìã Rapport g√©n√©r√© dans le r√©pertoire /opt/airflow/reports/")
        print("="*80)
        
    except Exception as e:
        # Gestion des erreurs avec information d√©taill√©e
        print("\n" + "="*80)
        print(f"‚ùå ERREUR DANS LE PIPELINE: {e}")
        print("üîç V√©rifiez les logs ci-dessus pour diagnostiquer le probl√®me")
        print("="*80)
        raise                                               # Re-lancement de l'exception pour Airflow


# =======================================================================================================================================
# POINT D'ENTR√âE DU SCRIPT
# =======================================================================================================================================

if __name__ == "__main__":
    # Ex√©cution du pipeline complet si le script est lanc√© directement
    main()
